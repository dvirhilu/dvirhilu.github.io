---
layout: default
---
  
<!doctype html>

<h1>
    Undergraduate Thesis - An Architecture to Allow Direct File-System Access on GPU
</h1>

<p>
    Starting in late January 2022, I joined Dr. Alexandra (Sasha) Fedorova's research group to complete an undergraduate thesis. This page contains a brief summary of the work, and the paper is embedded at the bottom.
</p>

<h2>
    Background Information
</h2>

<p>
    The group identified a bottle neck in many modern workloads which require massive parallelization and data handling: Many of these workloads operate largely on specialized processors (such as GPUs), and use a centralized CPU only to coordinate between subsystems and facilitate data transfers. Noting that many periferals today have their own processors (SSDs, smart NICs, memory), the group envisioned a decentralized, CPU-less system, in which independent, specialized devices communicate together to execute workloads.
</p>
<p>
    As a first step to this vision, the team wanted to explore the possibility of direct communication over shared memory between a GPU and a file system implemented in the SSD. As a proof of concept, a hardware emulator was used to simulate the external file system, and I worked on starting the implementation of direct file access on the GPU side.
</p>
<p>
    As I only had a few months for the thesis, I prioritized coming up with an architecture that accomplish the task on the GPU, and writing a simple proof of concept code for how this architecture that would interact with the hardware emulator.
</p>
<p>
    Before giving a brief explanation of the architecture, here is some basic, relevant information about the architecture of a GPU: 
</p>
<h3>
    GPU Hardware Architecture
</h3>

<img 
    src="images/GPU_arch.png" 
    alt="GPU hardware architecture" 
    style="width:800px"
>
    <figcaption>
        GPU hardware architecture.
    </figcaption>
</img>

<p>
    NVIDIA GPUs are made up of a large number of simple 32-core processors called <b>SMs</b>, with each core running its own <b>thread</b>. Under the CUDA programming model, threads are divided into <b>blocks</b>, with each block only running on a single SM for the entire lifetime of the program. If there are more than 32 threads in a block, they are further divided into 32-thread groups called <b>warps</b>. Threads in the same warp always execute concurently on the SM. All cores within a single SM run the same instruction. As a result, if threads within a warp do not execute the same instruction due to conditional branching, their execution is serialized. This is called <b>thread divergence</b>.
</p>

<p>
    In terms of memory architecture, NVIDIA GPUs have an L1 cache, and L2 cache, and RAM (there are also texture and constant memory but these were not used for this project). The L1 cache is located within the SMs, while the L2 and RAM are shared by all SMs. The L1 cache cache space is split, with half of it used as a proper cache, while the other half is available to the programmer and is called <b>shared memory</b>. Anything stored in shared memory is shared by and only available to threads within a specific block. Since shared memory is located inside the SM itself, it is smaller but significantly faster than RAM. Alternatively, the programmer can also store data in RAM, or <b>global memory</b>, which is shared across all SMs.
</p>

<h2>
    Summary of the Proposed Architecture
</h2>

<h3>
    Interface Layer
</h3>

<h3>
    Unified Memory
</h3>

<h3>
    FUSE Layer
</h3>

<h3>
    Driver Layer
</h3>

<h2>
    Example Flow for g_read Command
</h2>

<h2>
    Undergraduate Thesis Paper
</h2>

<object data="https://dvirhilu.github.io/projects/embedations/undergrad_thesis.pdf" type="application/pdf" width="1000px" height="1000px">
    <embed src="https://dvirhilu.github.io/projects/embedations/undergrad_thesis.pdf">
        <p>This browser does not support PDFs. Please download the PDF to view it: <a href="https://dvirhilu.github.io/projects/embedations/undergrad_thesis.pdf">Download PDF</a>.</p>
    </embed>
</object>