<!--Default HTML5 setup-->
<!DOCTYPE html>
<html lang="en">

<!--Defines metadata-->
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <!--Title for the page-->
    <title>Dvir Hilu Undergrad Thesis</title>
    <!--stylesheet reference-->
    <link rel="stylesheet" href="../css/globals.css">
    <link rel="stylesheet" href="../css/style.css">
    <!--icons reference-->
    <script src="https://kit.fontawesome.com/220fe8c60c.js" crossorigin="anonymous"></script>
</head>

<body>
    <!--Nav-->
    <div id="navContainer" class="nav-container flex-container">
        <i id="btnCloseNav" class="fa-solid fa-x flex-container" style="color: white"></i>
        <ul class="flex-container">
            <li><a href="index.html">Home Page</a></li>
            <li><a href="about-me.html">About Me</a></li>
            <li class="dropdown">
                <button class="dropdownbtn">Projects <i class="nav-arrow"></i></button>
                <div class="dropdown-content">
                    <a href="capstone1.html">Capstone 1</a>
                    <a href="multi_image_reconstruction.html">MIR</a>
                    <a href="undergrad_thesis.html">Undergrad Thesis</a>
                </div>
            </li>
        </ul>
    </div>
    <i id="btnOpenNav" class="fa-solid fa-bars open-nav flex-container hide" style="color: white"></i>


    <h1>
        Undergraduate Thesis - An Architecture to Allow Direct File-System Access on GPU
    </h1>

    <p>
        Starting in late January 2022, I joined Dr. Alexandra (Sasha) Fedorova's research group to complete an undergraduate
        thesis. This page contains a brief summary of the work, and the paper is embedded at the bottom.
    </p>

    <h2>
        Background Information
    </h2>

    <p>
        The group identified a bottle neck in many modern workloads which require massive parallelization and data handling: Many of these workloads operate largely on specialized hardware accelerators (such as GPUs), and use a centralized CPU only to coordinate between subsystems and facilitate data transfers. Noting that many peripherals today have their own processors (SSDs, smart NICs, memory), the group envisioned a decentralized, CPU-less system, in which independent devices communicate together to execute workloads.
    </p>
    <p>
        As a first step to this vision, the team wanted to explore the possibility of direct communication over unified memory between a GPU and a file system implemented in the SSD. As a proof of concept, a hardware emulator was used to simulate the external file system, and I started the implementation of direct file access on the GPU side (named VirtioFS). I only had a few months for the thesis, so I prioritized coming up with an architecture that can accomplish the task on the GPU, and writing simple proof of concept code for how this architecture that would interact with the hardware emulator.
    </p>
    <p>
        Before explaining the architecture, here are a few definitions relating to the GPU execution model that will be relevant for later sections: 
    </p>
    <h3>
        GPU Hardware Architecture Relevant Definitions
    </h3>

    <img src="images/GPU_arch.png" alt="GPU hardware architecture" style="width:800px">
    <figcaption>
        GPU hardware architecture.
    </figcaption>
    </img>

    <table>
        <tr>
            <th>Term</th>
            <th>Description</th>
        </tr>
        <tr>
            <td>Streaming Multiprocessors (SMs)</td>
            <td>NVIDIA's GPUs are comprised of a large number of these (typically 32-core) processors. All cores within an SM run the same instruction, though the data they operate on could be different. Each SM has its own L1 cache.</td>
        </tr>
        <tr>
            <td>Block</td>
            <td>Under the CUDA programming model, threads are grouped into blocks. Each block's threads run on a single SM for the entire lifetime of the program. Note that the number of threads in a block can be MUCH larger than the number of cores in an SM.</td>
        </tr>
        <tr>
            <td>Warp</td>
            <td>A block is further divided into warps, which match in size the number of cores in an SM. Threads in a warp always execute concurrently.</td>
        </tr>
        <tr>
            <td>Thread Divergence</td>
            <td>Due to the same-instruction model of an SM, if threads in the same warp don't execute the same instruction (i.e. conditional branching), their instruction execution is serialized (one branch executed after the other, NOT concurrently).</td>
        </tr>
        <tr>
            <td>Shared Memory</td>
            <td>Half of the L1 cache of an SM is allocated as "Shared Memory". Shared memory access is limited to threads in the same block, and provides a small, high-speed memory to use throughout the program.</td>
        </tr>
        <tr>
            <td>Global Memory</td>
            <td>Global memory is stored in the GPU RAM and can be accessed by ALL threads. As such, it is not thread safe.</td>
        </tr>
    </table>

    <h2>
        Summary of the Proposed Architecture for VirtioFS
    </h2>
    <img src="images/virtiofs_arch.png" alt="VirtioFS Architecture from the GPU's perspective." style="width:800px">
    <figcaption>
        VirtioFS Architecture from the GPU's perspective.
    </figcaption>
    </img>

    <p>

    </p>

    <h3>
        Interface Layer
    </h3>

    <p>
        The interface layer of VirtioFS, which is exposed to the user, provides the usual POSIX-like semantics for file
        system operations. The functions implemented for the proof-of-concept VirtioFS are shown in the table below:
    </p>

    <table>
        <tr>
            <th>API Function</th>
            <th>Description</th>
        </tr>
        <tr>
            <td>g_readdir</td>
            <td>open current directory and read its contents</td>
        </tr>
        <tr>
            <td>g_open</td>
            <td>open file and add its file descriptor to the current block's file table</td>
        </tr>
        <tr>
            <td>g_close</td>
            <td>close file and remove its file descriptor from the current block's file table</td>
        </tr>
        <tr>
            <td>g_fstat</td>
            <td>query the size of the file</td>
        </tr>
        <tr>
            <td>g_read</td>
            <td>read file and increment seek pointer by amount read</td>
        </tr>
        <tr>
            <td>g_write</td>
            <td>write to file and increment seek pointer by amount written</td>
        </tr>
        <tr>
            <td>g_lseek</td>
            <td>Change the position of the file's seek pointer</td>
        </tr>
        <tr>
            <td>g_dup2</td>
            <td>Duplicate the file descriptor, allowing to track separate seek pointers in the file</td>
        </tr>
    </table>

    <p>
        The major difference between the CPU file API and the VirtioFS interface is that VirtioFS does not allow threads to make file-system calls independently of each other. Doing this would result in thread-divergence, causing a huge bottle neck by serializing each file operation in the warp. Instead, VirtioFS requires that all threads within a warp call the same VirtioFS function with the exact same parameters, and utilizes the GPU's parallelism under the hood.
    </p>

    <h3>
        Unified Memory Allocation
    </h3>

    <p>
        The data transfer and communication between the GPU and SSD (or emulator) is done via unified memory, accessible
        from both devices. In work done before I joined, it was decided to follow <a
            href="https://docs.oasis-open.org/virtio/virtio/v1.1/csprd01/virtio-v1.1-csprd01.html#x1-30001">Virtio</a>, a
        standard for I/O communication used mostly in virtual machines. The communication protocol operates over unified
        memory, with a section dedicated to an I/O command queue (where Virtio commands are placed and the handshake
        happens. This will likely be replaced in the future with bus commands), and a unified memory buffer used for bulk
        data transfers.
    </p>

    <p>
        The unified memory buffer is further split into virtio queues (virtqueues) and command buffers. Virtqueues are a
        queue structure where device specific commands are placed. In this case, the virtqueues hold the file system
        specific commands. Any data that is transfered between the devices will be contained in the command buffers, and
        will be pointed to by the virtqueue entry. An example of the SSD would tranverse the unified memory on a read
        command is shown in the diagram below.
    </p>

    <img src="images/virtio_comm_flow.png" alt="How SSD might tranverse unified memory for read command"
        style="width:800px">
    <figcaption>
        Simplified view of how SSD might tranverse unified memory for read command. Starting at with the IO command, the
        device traverses the linked virtqueue entries and retrieves data from the command buffer.
    </figcaption>
    </img>

    <p>
        An important design decision I took here was to dedicate an individual virtqueue and buffer to each block in the
        program. If blocks were to access a shared virtqueue, accesses to the virtqueue would have to be synchronized.
        Allowing each block to have its own virtqueue and buffer space fully exploits the parallelism of the GPU and allows
        file system action to occur asynchronously from the perspective of the GPU.
    </p>

    <p>
        At initialization, each block is allocated the maximum transfer size negotiated by the SSD device. If the total
        allocated memory would exceed available memory, total memory is divided evenly amongst blocks. The obvious tradeoff
        here is that as the number of blocks increases, the largest amount of data in a single transfer is decreased. Note
        that while this approach is sufficient for the research teams applications of interest, CUDA in theory allows for a
        ridicuolously large number of blocks for each program. In the future, further research is required into how exactly
        to optimize between the partition size of unified memory and the number of blocks sharing a partition. That's an
        interesting question but I definitely did not have enough time to tackle this in the 5 months I worked on my thesis.
    </p>

    <h3>
        FUSE Layer
    </h3>

    <h3>
        Driver Layer
    </h3>

    <h2>
        Example Flow for g_read Command
    </h2>

    <h2>
        Undergraduate Thesis Paper
    </h2>

    <object data="https://dvirhilu.github.io/projects/embedations/undergrad_thesis.pdf" type="application/pdf"
        width="1000px" height="1000px">
        <embed src="https://dvirhilu.github.io/projects/embedations/undergrad_thesis.pdf">
        <p>This browser does not support PDFs. Please download the PDF to view it: <a
                href="https://dvirhilu.github.io/projects/embedations/undergrad_thesis.pdf">Download PDF</a>.</p>
        </embed>
    </object>

</body>